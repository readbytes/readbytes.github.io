<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Pandas for Data Science Real-World Data Science Projects with Pandas</title>
<link rel='shortcut icon' href='https://readbytes.github.io/images/favicon.ico'>
<link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet" />
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/codemirror.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/codemirror.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/theme/dracula.min.css"><style>
html {scroll-behavior: smooth;}  
body {font-family: 'Merriweather', serif;;margin: 0;padding: 2rem 1rem;font-size: 15px;line-height: 1.5;transition: background-color 0.3s, color 0.3s;}
.container {max-width: 750px;margin: 0 auto;padding: 1rem 2rem;}
.light-mode {background-color: #f8f7f5;color: #333;} 
.dark-mode {background-color: #121212;color: #e0e0e0;}
a { text-decoration: none;transition: color 0.2s;}
a:hover {text-decoration: underline;}
.light-mode a {color: #555;}
.dark-mode a {color: #aaa;}
</style><style>
.related-book-list {display: flex;flex-wrap: wrap;gap: 20px;}
.related-books {margin-top: 40px;}
.related-books h2 {font-size: 22px;margin-bottom: 20px;border-bottom: 2px solid #eee;padding-bottom: 8px;}
.light-mode .related-books h2 {color: #333;border-color: #eee;}
.dark-mode .related-books h2 {color: #ddd;border-color: #333;}
</style><style>
.book-cover {width: 130px;height: 180px;font-family: Arial, sans-serif;position: relative;overflow: hidden;box-shadow: 2px 2px 4px rgba(0,0,0,0.1);}
.book-cover-bottom-stripe {position: absolute;bottom: 0;left: 0;height: 5px;width: 100%;}
.book-cover-title {position: absolute;width: 100%;text-align: center;font-weight: bold;}
.book-cover-icon {position: absolute;left: 50%;transform: translateX(-50%);display: flex;align-items: center;justify-content: center;}
.book-cover-author {position: absolute;bottom: 10px;width: 100%;text-align: center;font-size: 8px;color: #333;}
</style><style>
.book-title-toc {font-size: 32px;font-weight: bold; }
.book-subtitle-toc {font-size: 18px;margin-top: 8px;font-style: italic;}
.book-header {text-align: center;margin-bottom: 40px;}

.toc-container {border-radius: 10px;padding: 30px;margin-bottom: 40px;box-shadow: 0 8px 20px rgba(0,0,0,0.05);}
h1 {font-size: 24px;text-align: center;margin-bottom: 30px;letter-spacing: 1px;padding-bottom: 10px;}
.toc-list,.toc-section-list {list-style: none;padding: 0;margin: 0;}
.toc-section-list {padding-left: 18px;margin-top: 6px;margin-bottom: 20px; }
.toc-chapter {padding: 20px 0;}
.chapter-title,.toc-section {display: flex;justify-content: space-between;padding-top: 4px;}
.chapter-title {font-size: 17px;font-weight: bold;}
.chapter-page,.section-page {font-style: italic;}
 
 
.light-mode .book-title-toc,.light-mode .chapter-title,.light-mode .book-name {color: #222;}
.light-mode .book-subtitle-toc,.light-mode .book-sub,.light-mode .chapter-page,.light-mode .section-page {color: #666;}
.light-mode .toc-container { background: #fff;border: 1px solid #ddd;}
.light-mode h1 {border-bottom: 2px solid #eee;color: #222;}
.light-mode .toc-section {  color: #444;}

.light-mode .book-cover { border: 1px solid #ccc;box-shadow: 0 6px 12px rgba(0, 0, 0, 0.15);}
.light-mode .book-title,.light-mode .series-name {color: #fff;}
.light-mode .book-subtitle,.light-mode .book-footer {color: #aaa;}

.dark-mode .book-title-toc,.dark-mode .chapter-title,.dark-mode .book-name {color: #f5f5f5;}
.dark-mode .book-subtitle-toc,.dark-mode .book-sub,.dark-mode .chapter-page,.dark-mode .section-page {color: #aaa;}
.dark-mode .toc-container {background: #1e1e1e;border: 1px solid #444;}

.dark-mode h1 {border-bottom: 2px solid #333;color: #eee;}
.dark-mode .toc-section {color: #bbb;}

.dark-mode .related-books h2 {color: #ddd;border-bottom: 2px solid #333;}
.dark-mode .book-title,.dark-mode .series-name {color: #fff;}
.dark-mode .book-subtitle,.dark-mode .book-footer {color: #bbb;}
.dark-mode .toc-section {color: #ddd;}

@media (max-width: 600px) {
  .chapter-title,
  .toc-section {
    flex-direction: column;
    align-items: flex-start;
  }

  .chapter-page,
  .section-page {
    margin-top: 4px;
  }
}

</style><style>
.light-mode .container {box-shadow: 0 0 15px rgba(0,0,0,0.1);}
.dark-mode .container {box-shadow: 0 0 15px rgba(255, 255, 255, 0.05);}

h1, h2, h3 {font-weight: 600;margin-top: 2.5rem;margin-bottom: 1rem;line-height: 1.2;}
h1 { font-size: 1.8rem; }
h2 { font-size: 1.4rem; }
h3 { font-size: 1.2rem; }

table {width: 100%;border-collapse: collapse;font-family: sans-serif;font-size: 1em;margin: 1em 0;}
th, td {padding: 0.6em 1em;text-align: left;border: 1px solid;}
tr:nth-child(even) {  background-color: inherit;}
tr:hover { background-color: inherit;}

code {color: #d35400;}

.chapter-navi-section {display: flex;justify-content: space-between;align-items: center;padding-bottom: 10px;padding-top: 10px;}
.nav-link {color: inherit;text-decoration: none;opacity: 0.5;transition: opacity 0.3s ease, text-decoration 0.3s ease;}
.nav-link:hover {opacity: 0.9;text-decoration: underline;}
.prev { text-align: left; }
.toc-link { text-align: center; }
.next { text-align: right; }

.download-section {display: flex;align-items: center;justify-content: center;gap: 2rem;padding: 2rem 1rem;max-width: 600px;margin: 3rem auto;border-radius: 12px;box-shadow: 0 4px 10px rgba(0,0,0,0.05);}
.download-info h2 {margin: 0 0 0.5rem;font-size: 1.2rem;}
.format-label {margin: 0 0 0.5rem;font-size: 0.95rem;}
.download-buttons {display: flex;gap: 1rem;}
.download-button {background-color: #4A90E2;color: white;padding: 0.5em 1.2em;border-radius: 25px;font-size: 0.95rem;text-decoration: none;transition: background-color 0.3s ease;}
.download-button:hover {background-color: #357ABD;}

.code-block {position: relative;margin-bottom: 1em;}
.copy-button {background: transparent;color: white;position: absolute;top: 8px;right: 8px;border: none;padding: 4px 8px;font-size: 12px;border-radius: 4px;cursor: pointer;z-index: 1;}
.copy-button:hover {background-color: #0056b3;}

.snippet-container {overflow: hidden;}
.snippet-header {padding: 0.75rem 1rem;cursor: pointer;font-size: 1rem;background-color: #eeeeee;color:  #111111; border-radius: 8px;cursor: pointer;box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
.snippet-body {display: none;font-family: monospace;}
.snippet-container.open .snippet-body {display: block;}

.CodeMirror {border: 2px solid #ccc;border-radius: 4px;resize: vertical;}

.light-mode .format-label {color: #888;}
.light-mode th {background-color: #f2f2f2;}
.light-mode tr:nth-child(even) {background-color: #fafafa;}
.light-mode tr:hover {background-color: #e6f7ff;}

.dark-mode .snippet-header {background-color: #333;color:  #ddd; }
.dark-mode .format-label {color: #aaa;}
.dark-mode th {background-color: #333;color: #fff;}
.dark-mode td {border-color: #444;}
.dark-mode tr:nth-child(even) {background-color: #2a2a2a;}
.dark-mode tr:hover {background-color: #2c3e50;}


.button-group-run-edit {display: flex;width: 100%;gap: 1rem;}
.button-run-edit {flex: 1;padding: 1rem;font-size: 1rem;font-weight: 600;border: none;border-radius: 8px;cursor: pointer;transition: background 0.3s, color 0.3s;}

.light-mode .button-run-edit {background-color: #f0f0f0;color: #222;box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);}
.light-mode .button-run-edit:hover {background-color: #e0e0e0;}

.dark-mode .button-run-edit {background-color: #2a2a2a;color: #f5f5f5;box-shadow: 0 0 10px rgba(255, 255, 255, 0.05);}
.dark-mode .button-run-edit:hover {background-color: #3a3a3a;}

button { padding: 0.5em 1em; background: green; color: white; border: none; cursor: pointer; }
button:hover { background: #1e7e34; }
.result_iframe { width:99%; margin-top: 1em;  resize: both;background-color: #515b63;border:1px solid #ccc;}   

.dialog-backdrop {position: fixed;top: 0;left: 0;width: 100vw;height: 100vh;background: rgba(0,0,0,0.5);display: none;justify-content: center;align-items: center;z-index: 10;}
.dialog {background: #90979e;padding: 20px;width: 90vw;height: 80vh;border-radius: 8px;box-shadow: 0 0 20px rgba(0,0,0,0.3);display: flex;flex-direction: column;}
.dialog-content {display: flex;gap: 20px;margin-top: 10px;height: calc(100% - 40px);}
.editor-container, .preview-container {flex: 1;height: 100%;max-height: 100%;overflow: hidden; }
.result_iframe_dialog {width: 100%;height: 100%;resize: both;border:1px solid #ccc;overflow: auto; }   
.CodeMirror {width: 100% !important;height: 100% !important;}
</style>
</head>
<body>
<script>const mode = localStorage.getItem('mode') || 'light';document.documentElement.classList.add(`${mode}-mode`);</script>  
<div class="container"><div class = "chapter-navi-section">
<a href="pandas-for-data-science-customizing-pandas-with-extensions-and-apis.htm" class="nav-button prev">←</a>
<a href='pandas-for-data-science.htm#real-world-data-science-projects-with-pandas' class="nav-button toc-link">Index</a>
<a href="" class="nav-button next"></a>
</div>
<div class='book-header'><h1 class='book-title-toc' id = 'top'>Real-World Data Science Projects with Pandas</h1><h3 class='book-subtitle-toc'>Pandas for Data Science</h3></div><h2 id='project-1-exploratory-data-analysis-on-a-public-dataset'>15.1 Project 1: Exploratory Data Analysis on a Public Dataset</h2><p>Exploratory Data Analysis (EDA) is a crucial first step in any data science project. It helps you understand the dataset’s structure, detect anomalies, and generate hypotheses. In this project, we’ll walk through EDA on the classic Titanic dataset using pandas. The dataset contains passenger details such as age, sex, ticket class, and survival status — perfect for demonstrating pandas’ power in data exploration.</p><h3 id="step-1-loading-the-dataset">Step 1: Loading the Dataset</h3><p>First, import pandas and load the Titanic dataset. You can get it directly from the seaborn library for convenience:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load Titanic dataset from seaborn
df = sns.load_dataset('titanic')
print(df.head())</code></pre>
</div><p>This dataset has columns like <code>survived</code>, <code>pclass</code>, <code>sex</code>, <code>age</code>, and more. Let’s start by understanding the shape and basic info.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">print(f"Dataset shape: {df.shape}")
print(df.info())</code></pre>
</div><h3 id="step-2-data-cleaning-and-handling-missing-values">Step 2: Data Cleaning and Handling Missing Values</h3><p>Check for missing values using <code>.isna()</code> and <code>.sum()</code>:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">print(df.isna().sum())</code></pre>
</div><p>You’ll notice significant missing data in <code>age</code>, <code>deck</code>, and <code>embarked</code>. For simplicity:</p><ul>
 <li>Fill missing <code>age</code> values with the median age.</li>
 <li>Drop the <code>deck</code> column due to too many missing values.</li>
 <li>Fill missing <code>embarked</code> values with the mode.</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">df['age'].fillna(df['age'].median(), inplace=True)
df.drop(columns=['deck'], inplace=True)
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)</code></pre>
</div><p>Verify no missing data remains in key columns:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">print(df[['age', 'deck', 'embarked']].isna().sum())</code></pre>
</div><h3 id="step-3-summarizing-the-data">Step 3: Summarizing the Data</h3><p>Use <code>.describe()</code> to get statistical summaries of numeric columns:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">print(df.describe())</code></pre>
</div><p>This reveals the distribution of age, fare, and more. For categorical data, use <code>.value_counts()</code>:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">print(df['sex'].value_counts())
print(df['pclass'].value_counts())
print(df['embarked'].value_counts())</code></pre>
</div><p>Understanding the distribution of classes and embarkation points provides insights into passenger demographics.</p><h3 id="step-4-grouping-and-aggregation">Step 4: Grouping and Aggregation</h3><p>Now, let’s analyze survival rates by different groups:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Survival rate overall
print(f"Overall survival rate: {df['survived'].mean():.2f}")

# Survival rate by sex
print(df.groupby('sex')['survived'].mean())

# Survival rate by passenger class
print(df.groupby('pclass')['survived'].mean())</code></pre>
</div><p>Notice, for example, females have a much higher survival rate than males. Also, 1st-class passengers survived more often than those in lower classes.</p><h3 id="step-5-visualizing-data">Step 5: Visualizing Data</h3><p>Visualizations provide intuitive insights.</p><h4 id="survival-by-sex-and-class">Survival by Sex and Class</h4><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">sns.barplot(data=df, x='sex', y='survived')
plt.title('Survival Rate by Sex')
plt.show()

sns.barplot(data=df, x='pclass', y='survived')
plt.title('Survival Rate by Passenger Class')
plt.show()</code></pre>
</div><h4 id="age-distribution-by-survival">Age Distribution by Survival</h4><p>Use histograms to compare age distributions:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">plt.figure(figsize=(10,5))
sns.histplot(data=df, x='age', hue='survived', multiple='stack', bins=30)
plt.title('Age Distribution by Survival')
plt.show()</code></pre>
</div><p>You can see that younger passengers had a higher chance of survival.</p><h4 id="correlation-heatmap">Correlation Heatmap</h4><p>Check correlations between numerical features:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()</code></pre>
</div><p>Note positive correlations between <code>fare</code> and <code>pclass</code> (inverse relationship due to coding), and a negative correlation between age and survival.</p><h3 id="step-6-insights-and-next-steps">Step 6: Insights and Next Steps</h3><ul>
 <li><strong>Key Insights</strong>: Female passengers and first-class travelers had higher survival chances. Younger passengers also tended to survive more.</li>
 <li><strong>Missing Data</strong>: Handling missing ages by median is simple but can be improved using more advanced imputation.</li>
 <li><strong>Feature Engineering</strong>: Adding features like family size (<code>sibsp + parch</code>), title extraction from names, or fare bins can enhance predictive modeling.</li>
 <li><strong>Next Steps</strong>: After EDA, prepare data for machine learning by encoding categorical variables, scaling, and splitting into training/testing sets.</li>
</ul><h3 id="full-runnable-code-summary">Full Runnable Code Summary</h3><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = sns.load_dataset('titanic')

# Data overview
print(df.info())
print(df.isna().sum())

# Cleaning missing values
df['age'].fillna(df['age'].median(), inplace=True)
df.drop(columns=['deck'], inplace=True)
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)

# Summary statistics
print(df.describe())
print(df['sex'].value_counts())

# Grouping and aggregation
print(df.groupby('sex')['survived'].mean())
print(df.groupby('pclass')['survived'].mean())

# Visualizations
sns.barplot(data=df, x='sex', y='survived')
plt.title('Survival Rate by Sex')
plt.show()

sns.barplot(data=df, x='pclass', y='survived')
plt.title('Survival Rate by Passenger Class')
plt.show()

plt.figure(figsize=(10,5))
sns.histplot(data=df, x='age', hue='survived', multiple='stack', bins=30)
plt.title('Age Distribution by Survival')
plt.show()

corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()</code></pre>
</div><h3 id="conclusion">Conclusion</h3><p>This project demonstrated a typical EDA workflow using pandas: loading, cleaning, summarizing, grouping, and visualizing data. Through incremental steps, we uncovered meaningful patterns that guide further modeling or business decisions. With pandas’ rich functionality, EDA becomes a smooth and powerful experience — essential for every data scientist.</p><div class = "chapter-navi-section">
<a href="#top" class="nav-button prev">↑</a>
<a href='pandas-for-data-science.htm#real-world-data-science-projects-with-pandas' class="nav-button toc-link">Index</a>
<a href="#bottom" class="nav-button next">↓</a>
</div>
<h2 id='project-2-time-series-forecasting-preparation'>15.2 Project 2: Time Series Forecasting Preparation</h2><p>Preparing time series data properly is essential for building accurate forecasting models. This project walks you through the key steps of handling datetime indexes, resampling data, managing missing values, and engineering time-based features, all using a practical stock price dataset. These preprocessing steps ensure your data is clean, consistent, and enriched for modeling.</p><h3 id="step-1-loading-and-inspecting-the-data">Step 1: Loading and Inspecting the Data</h3><p>We’ll use historical stock price data for a well-known company, Apple (AAPL), from Yahoo Finance, accessible via <code>yfinance</code> or CSV downloads. For this example, let’s load a CSV file with daily stock prices.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import matplotlib.pyplot as plt

# Load dataset (assuming CSV downloaded locally or from URL)
df = pd.read_csv('AAPL.csv', parse_dates=['Date'])
print(df.head())</code></pre>
</div><p>The dataset typically includes columns like <code>Date</code>, <code>Open</code>, <code>High</code>, <code>Low</code>, <code>Close</code>, <code>Volume</code>, and sometimes <code>Adj Close</code>.</p><h3 id="step-2-setting-the-datetime-index">Step 2: Setting the DateTime Index</h3><p>Time series analysis requires a datetime index. Set the <code>Date</code> column as the DataFrame index and sort it:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">df.set_index('Date', inplace=True)
df.sort_index(inplace=True)
print(df.index)</code></pre>
</div><p>Having a <code>DateTimeIndex</code> enables powerful time-aware operations like resampling and rolling calculations.</p><h3 id="step-3-handling-missing-data">Step 3: Handling Missing Data</h3><p>Real-world time series often have missing dates or data points. For example, stock markets close on weekends, so daily data may miss weekends.</p><p>Check for missing dates by creating a full date range and comparing:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">full_idx = pd.date_range(start=df.index.min(), end=df.index.max(), freq='B')  # 'B' for business days
missing_dates = full_idx.difference(df.index)
print(f"Missing business days: {missing_dates}")</code></pre>
</div><p>To fill missing dates with <code>NaN</code>, reindex the DataFrame:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">df = df.reindex(full_idx)
print(df.head(10))</code></pre>
</div><h3 id="step-4-resampling-the-data">Step 4: Resampling the Data</h3><p>You may want to change the frequency from daily to weekly or monthly for smoothing or reducing noise.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Resample to monthly frequency, taking mean of 'Close' price
monthly_close = df['Close'].resample('M').mean()
monthly_close.plot(title='Monthly Average Close Price')
plt.show()</code></pre>
</div><p>Or downsample to weekly:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">weekly_close = df['Close'].resample('W').last()
weekly_close.plot(title='Weekly Close Price')
plt.show()</code></pre>
</div><p>For upsampling (e.g., from monthly to daily), interpolation methods fill gaps:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">daily_upsampled = monthly_close.resample('D').interpolate()
daily_upsampled.plot(title='Daily Upsampled Close Price (Interpolated)')
plt.show()</code></pre>
</div><h3 id="step-5-filling-missing-values">Step 5: Filling Missing Values</h3><p>After reindexing or resampling, fill missing values sensibly. Common methods include forward fill (<code>ffill</code>) or interpolation:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">df['Close'].fillna(method='ffill', inplace=True)  # propagate last valid observation forward
df['Close'].fillna(method='bfill', inplace=True)  # or backward fill if needed</code></pre>
</div><p>Avoid dropping missing data blindly, especially in time series where continuity matters.</p><h3 id="step-6-feature-engineering-for-time-series">Step 6: Feature Engineering for Time Series</h3><p>Adding features based on time can improve forecasting models. Common time features include:</p><ul>
 <li><strong>Lag features:</strong> previous day’s or week’s values</li>
 <li><strong>Rolling statistics:</strong> moving averages, moving standard deviations</li>
 <li><strong>Date components:</strong> day of week, month, quarter, year</li>
</ul><p>Example: create lag and rolling mean features:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">df['Close_lag1'] = df['Close'].shift(1)
df['Close_rolling7'] = df['Close'].rolling(window=7).mean()</code></pre>
</div><p>Extract date components:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6
df['month'] = df.index.month
df['quarter'] = df.index.quarter
df['year'] = df.index.year</code></pre>
</div><h3 id="step-7-exploratory-visualization">Step 7: Exploratory Visualization</h3><p>Visualize the original and engineered features:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">plt.figure(figsize=(12,6))
plt.plot(df['Close'], label='Close Price')
plt.plot(df['Close_rolling7'], label='7-Day Rolling Mean', linestyle='--')
plt.title('Close Price and 7-Day Rolling Mean')
plt.legend()
plt.show()</code></pre>
</div><p>Plot lagged vs current values to check correlations:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">plt.scatter(df['Close_lag1'], df['Close'])
plt.xlabel('Close Price (t-1)')
plt.ylabel('Close Price (t)')
plt.title('Lagged Close vs Current Close')
plt.show()</code></pre>
</div><h3 id="step-8-summary-and-next-steps">Step 8: Summary and Next Steps</h3><ul>
 <li><strong>Datetime Index:</strong> Enables time-aware operations.</li>
 <li><strong>Resampling:</strong> Flexibly change time granularity.</li>
 <li><strong>Handling Missing Data:</strong> Use interpolation or filling rather than dropping.</li>
 <li><strong>Feature Engineering:</strong> Create lag, rolling stats, and date features to capture temporal patterns.</li>
 <li><strong>Visualization:</strong> Confirm trends and stationarity visually.</li>
</ul><p>This cleaned and enriched dataset is now ready for time series forecasting models like ARIMA, Prophet, or machine learning regressors.</p><h3 id="full-runnable-code-snippet-summary">Full Runnable Code Snippet Summary</h3><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import matplotlib.pyplot as plt

# Load and prepare data
df = pd.read_csv('AAPL.csv', parse_dates=['Date'])
df.set_index('Date', inplace=True)
df.sort_index(inplace=True)

# Reindex to business days
full_idx = pd.date_range(df.index.min(), df.index.max(), freq='B')
df = df.reindex(full_idx)

# Fill missing values
df['Close'].fillna(method='ffill', inplace=True)
df['Close'].fillna(method='bfill', inplace=True)

# Resample monthly average close
monthly_close = df['Close'].resample('M').mean()
monthly_close.plot(title='Monthly Average Close Price')
plt.show()

# Feature engineering
df['Close_lag1'] = df['Close'].shift(1)
df['Close_rolling7'] = df['Close'].rolling(window=7).mean()
df['day_of_week'] = df.index.dayofweek
df['month'] = df.index.month

# Visualization
plt.figure(figsize=(12,6))
plt.plot(df['Close'], label='Close Price')
plt.plot(df['Close_rolling7'], label='7-Day Rolling Mean', linestyle='--')
plt.title('Close Price and 7-Day Rolling Mean')
plt.legend()
plt.show()</code></pre>
</div><div class = "chapter-navi-section">
<a href="#top" class="nav-button prev">↑</a>
<a href='pandas-for-data-science.htm#real-world-data-science-projects-with-pandas' class="nav-button toc-link">Index</a>
<a href="#bottom" class="nav-button next">↓</a>
</div>
<h2 id='project-3-customer-segmentation-with-grouping-and-aggregation'>15.3 Project 3: Customer Segmentation with Grouping and Aggregation</h2><p>Customer segmentation is a key task in marketing analytics, helping businesses tailor campaigns and improve customer engagement by grouping customers based on purchasing behavior. In this project, we’ll simulate a purchase dataset and demonstrate how to use pandas’ <code>groupby()</code>, aggregation, filtering, and sorting functions to segment customers. Finally, we’ll visualize the results to extract actionable marketing insights.</p><h3 id="step-1-simulating-purchase-data">Step 1: Simulating Purchase Data</h3><p>Let's create a sample dataset representing customer purchases over several months. Each record includes customer ID, purchase date, product category, and amount spent.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Seed for reproducibility
np.random.seed(42)

# Create sample data
n_customers = 100
n_purchases = 1000

customer_ids = np.random.choice(range(1, n_customers + 1), size=n_purchases)
dates = pd.date_range(start='2023-01-01', periods=180).to_numpy()
purchase_dates = np.random.choice(dates, size=n_purchases)
categories = np.random.choice(['Electronics', 'Clothing', 'Groceries', 'Books'], size=n_purchases)
amounts = np.round(np.random.exponential(scale=100, size=n_purchases), 2)

# Create DataFrame
df = pd.DataFrame({
    'CustomerID': customer_ids,
    'PurchaseDate': purchase_dates,
    'Category': categories,
    'Amount': amounts
})

print(df.head())</code></pre>
</div><h3 id="step-2-creating-summary-metrics-via-groupby-and-aggregation">Step 2: Creating Summary Metrics via GroupBy and Aggregation</h3><p>To segment customers, we first summarize their purchasing behavior:</p><ul>
 <li>Total spending</li>
 <li>Number of purchases</li>
 <li>Average purchase amount</li>
 <li>Most frequent purchase category</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Group by CustomerID
customer_summary = df.groupby('CustomerID').agg(
    Total_Spent=pd.NamedAgg(column='Amount', aggfunc='sum'),
    Purchase_Count=pd.NamedAgg(column='Amount', aggfunc='count'),
    Avg_Purchase=pd.NamedAgg(column='Amount', aggfunc='mean')
).reset_index()

# Determine each customer's most frequent category
mode_category = df.groupby('CustomerID')['Category'] \
                  .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)

customer_summary['Top_Category'] = mode_category.values

print(customer_summary.head())</code></pre>
</div><h3 id="step-3-segmenting-customers">Step 3: Segmenting Customers</h3><p>A simple segmentation approach divides customers based on spending and purchase frequency:</p><ul>
 <li><strong>High Value</strong>: Total spent &gt; $500</li>
 <li><strong>Frequent Buyers</strong>: Purchase count &gt; 10</li>
 <li><strong>Occasional</strong>: Otherwise</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">def segment_customer(row):
    if row['Total_Spent'] &gt; 500:
        return 'High Value'
    elif row['Purchase_Count'] &gt; 10:
        return 'Frequent Buyer'
    else:
        return 'Occasional'

customer_summary['Segment'] = customer_summary.apply(segment_customer, axis=1)
print(customer_summary['Segment'].value_counts())</code></pre>
</div><h3 id="step-4-filtering-and-sorting-segments">Step 4: Filtering and Sorting Segments</h3><p>Identify the top 5 high-value customers by total spend:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">top_high_value = customer_summary[customer_summary['Segment'] == 'High Value'] \
    .sort_values(by='Total_Spent', ascending=False) \
    .head(5)

print("Top 5 High-Value Customers:")
print(top_high_value[['CustomerID', 'Total_Spent', 'Purchase_Count', 'Top_Category']])</code></pre>
</div><h3 id="step-5-visualizing-customer-segments">Step 5: Visualizing Customer Segments</h3><p>Visualizations help communicate segment characteristics:</p><ul>
 <li>Bar plot of segment counts</li>
 <li>Boxplot of spending by segment</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">plt.figure(figsize=(10,4))

# Bar plot of segment sizes
plt.subplot(1, 2, 1)
customer_summary['Segment'].value_counts().plot(kind='bar', color=['green', 'blue', 'orange'])
plt.title('Customer Segment Counts')
plt.ylabel('Number of Customers')

# Boxplot of total spent by segment
plt.subplot(1, 2, 2)
customer_summary.boxplot(column='Total_Spent', by='Segment', grid=False)
plt.title('Total Spending by Segment')
plt.suptitle('')  # Remove automatic title
plt.ylabel('Total Spent ($)')

plt.tight_layout()
plt.show()</code></pre>
</div><h3 id="step-6-interpretation-and-marketing-insights">Step 6: Interpretation and Marketing Insights</h3><ul>
 <li><strong>High Value</strong> customers contribute a large share of revenue but may be fewer in number.</li>
 <li><strong>Frequent Buyers</strong> show loyalty and regular engagement, valuable for subscription or upselling strategies.</li>
 <li><strong>Occasional</strong> customers might need targeted promotions or incentives to increase frequency.</li>
</ul><p>Analyzing the top categories per segment can tailor campaigns. For example, if <strong>High Value</strong> customers mostly buy electronics, personalized offers on new gadgets may increase retention.</p><h3 id="full-runnable-code-summary">Full Runnable Code Summary</h3><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

n_customers = 100
n_purchases = 1000
customer_ids = np.random.choice(range(1, n_customers + 1), size=n_purchases)
dates = pd.date_range(start='2023-01-01', periods=180).to_numpy()
purchase_dates = np.random.choice(dates, size=n_purchases)
categories = np.random.choice(['Electronics', 'Clothing', 'Groceries', 'Books'], size=n_purchases)
amounts = np.round(np.random.exponential(scale=100, size=n_purchases), 2)

df = pd.DataFrame({
    'CustomerID': customer_ids,
    'PurchaseDate': purchase_dates,
    'Category': categories,
    'Amount': amounts
})

# Summarize customer purchase behavior
customer_summary = df.groupby('CustomerID').agg(
    Total_Spent=pd.NamedAgg(column='Amount', aggfunc='sum'),
    Purchase_Count=pd.NamedAgg(column='Amount', aggfunc='count'),
    Avg_Purchase=pd.NamedAgg(column='Amount', aggfunc='mean')
).reset_index()

mode_category = df.groupby('CustomerID')['Category'] \
                  .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)
customer_summary['Top_Category'] = mode_category.values

# Define segments
def segment_customer(row):
    if row['Total_Spent'] &gt; 500:
        return 'High Value'
    elif row['Purchase_Count'] &gt; 10:
        return 'Frequent Buyer'
    else:
        return 'Occasional'

customer_summary['Segment'] = customer_summary.apply(segment_customer, axis=1)

# Top 5 High Value customers
top_high_value = customer_summary[customer_summary['Segment'] == 'High Value'] \
    .sort_values(by='Total_Spent', ascending=False).head(5)

print(top_high_value[['CustomerID', 'Total_Spent', 'Purchase_Count', 'Top_Category']])

# Visualization
plt.figure(figsize=(10,4))
plt.subplot(1, 2, 1)
customer_summary['Segment'].value_counts().plot(kind='bar', color=['green', 'blue', 'orange'])
plt.title('Customer Segment Counts')
plt.ylabel('Number of Customers')

plt.subplot(1, 2, 2)
customer_summary.boxplot(column='Total_Spent', by='Segment', grid=False)
plt.title('Total Spending by Segment')
plt.suptitle('')
plt.ylabel('Total Spent ($)')
plt.tight_layout()
plt.show()</code></pre>
</div><h3 id="conclusion">Conclusion</h3><p>This project demonstrates how pandas’ powerful grouping and aggregation tools enable meaningful customer segmentation. By summarizing purchase behaviors and adding domain-specific segmentation logic, businesses can discover actionable insights and improve marketing effectiveness. Visualizations further support data-driven decisions and communication with stakeholders.</p><p>Ready for Project 4 on data cleaning and merging?</p><div class = "chapter-navi-section">
<a href="#top" class="nav-button prev">↑</a>
<a href='pandas-for-data-science.htm#real-world-data-science-projects-with-pandas' class="nav-button toc-link">Index</a>
<a href="#bottom" class="nav-button next">↓</a>
</div>
<h2 id='project-4-data-cleaning-and-merging-from-multiple-sources'>15.4 Project 4: Data Cleaning and Merging from Multiple Sources</h2><p>In real-world data science projects, data rarely comes from a single, clean source. Instead, datasets originate from multiple formats and systems — CSV files, databases, APIs — each with inconsistencies, missing values, or overlapping information. This project walks through combining such disparate data sources, cleaning them, handling missing values, and producing a unified dataset ready for analysis.</p><h3 id="step-1-loading-multiple-datasets">Step 1: Loading Multiple Datasets</h3><p>Suppose we have customer purchase data from two different branches of a store, each stored as a CSV file. Additionally, we have a product catalog stored in JSON format.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import numpy as np

# Load sales data from two CSVs
branch1 = pd.read_csv('branch1_sales.csv')
branch2 = pd.read_csv('branch2_sales.csv')

# Load product catalog from JSON
products = pd.read_json('products.json')

# Show sample data
print(branch1.head())
print(branch2.head())
print(products.head())</code></pre>
</div><h3 id="step-2-inspecting-and-cleaning-individual-dataframes">Step 2: Inspecting and Cleaning Individual DataFrames</h3><p>Before merging, inspect the data for inconsistencies:</p><ul>
 <li>Check column names and data types</li>
 <li>Identify missing values</li>
 <li>Standardize formats (e.g., date formats, string capitalization)</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">print(branch1.info())
print(branch2.info())

# Convert purchase dates to datetime
branch1['PurchaseDate'] = pd.to_datetime(branch1['PurchaseDate'], errors='coerce')
branch2['PurchaseDate'] = pd.to_datetime(branch2['PurchaseDate'], errors='coerce')

# Normalize product IDs and customer IDs to string for consistency
branch1['ProductID'] = branch1['ProductID'].astype(str)
branch2['ProductID'] = branch2['ProductID'].astype(str)

branch1['CustomerID'] = branch1['CustomerID'].astype(str)
branch2['CustomerID'] = branch2['CustomerID'].astype(str)

# Standardize string columns (e.g., product names)
products['ProductName'] = products['ProductName'].str.strip().str.title()</code></pre>
</div><h3 id="step-3-combining-sales-data-from-branches">Step 3: Combining Sales Data from Branches</h3><p>Use <code>pd.concat()</code> to stack branch data vertically since they share the same schema.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">sales = pd.concat([branch1, branch2], ignore_index=True)
print(f"Combined sales records: {sales.shape[0]}")</code></pre>
</div><h3 id="step-4-handling-duplicates-and-missing-data">Step 4: Handling Duplicates and Missing Data</h3><p>After concatenation, some transactions might be duplicated due to overlapping data or data entry errors.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Detect duplicates based on key columns
duplicates = sales.duplicated(subset=['TransactionID'])
print(f"Number of duplicate transactions: {duplicates.sum()}")

# Remove duplicates, keeping the first occurrence
sales = sales.drop_duplicates(subset=['TransactionID'])</code></pre>
</div><p>Missing data is also common; handle it based on context:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Check for missing values
print(sales.isnull().sum())

# For missing purchase dates, drop rows or fill if possible
sales = sales.dropna(subset=['PurchaseDate'])

# For missing amounts, fill with zero or median depending on business logic
sales['Amount'] = sales['Amount'].fillna(sales['Amount'].median())</code></pre>
</div><h3 id="step-5-merging-sales-with-product-catalog">Step 5: Merging Sales with Product Catalog</h3><p>To enrich sales data with product details, perform a merge on the <code>ProductID</code> key.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Merge sales with products on ProductID
full_data = sales.merge(products, on='ProductID', how='left', indicator=True)

# Check for unmatched products
unmatched = full_data[full_data['_merge'] == 'left_only']
print(f"Unmatched product IDs: {unmatched['ProductID'].nunique()}")

# Depending on analysis needs, handle unmatched products:
# Option 1: Drop unmatched records
full_data = full_data[full_data['_merge'] == 'both']

# Option 2: Investigate and fix product catalog or sales data errors</code></pre>
</div><h3 id="step-6-validating-and-finalizing-the-dataset">Step 6: Validating and Finalizing the Dataset</h3><p>Validation ensures data quality before analysis:</p><ul>
 <li>Confirm no missing critical fields (e.g., customer ID, amount)</li>
 <li>Validate data ranges (e.g., amounts &gt; 0)</li>
 <li>Check data types</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Drop rows with missing customer IDs
full_data = full_data.dropna(subset=['CustomerID'])

# Remove records with negative or zero amounts
full_data = full_data[full_data['Amount'] &gt; 0]

# Final info
print(full_data.info())</code></pre>
</div><h3 id="step-7-summary-and-export">Step 7: Summary and Export</h3><p>After cleaning and merging, save the unified dataset for downstream analysis:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">full_data.to_csv('cleaned_sales_data.csv', index=False)</code></pre>
</div><h3 id="recap-and-best-practices">Recap and Best Practices</h3><ul>
 <li><strong>Consistent formats:</strong> Normalize IDs and datetime fields early.</li>
 <li><strong>Concatenate similar data:</strong> Use <code>concat()</code> to stack datasets with same schema.</li>
 <li><strong>Merge to enrich:</strong> Use <code>merge()</code> for combining on keys; choose join types (<code>inner</code>, <code>left</code>) carefully.</li>
 <li><strong>Handle duplicates:</strong> Use <code>duplicated()</code> and <code>drop_duplicates()</code> to remove repeats.</li>
 <li><strong>Address missing data:</strong> Tailor filling or dropping strategies based on domain knowledge.</li>
 <li><strong>Validate post-merge:</strong> Check for mismatches and data integrity.</li>
 <li><strong>Document transformations:</strong> Keep track of cleaning steps for reproducibility.</li>
</ul><h3 id="full-code-example-simulated-data">Full Code Example (Simulated Data)</h3><p>Below is a runnable minimal example using simulated data mimicking the above workflow:</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import numpy as np

# Simulate branch sales
branch1 = pd.DataFrame({
    'TransactionID': ['T1', 'T2', 'T3'],
    'CustomerID': ['C1', 'C2', 'C3'],
    'ProductID': ['P1', 'P2', 'P3'],
    'PurchaseDate': ['2023-01-01', '2023-01-02', '2023-01-03'],
    'Amount': [100, 150, 200]
})

branch2 = pd.DataFrame({
    'TransactionID': ['T4', 'T2', 'T5'],  # Notice T2 duplicate
    'CustomerID': ['C4', 'C2', 'C5'],
    'ProductID': ['P1', 'P2', 'P4'],
    'PurchaseDate': ['2023-01-04', '2023-01-02', None],
    'Amount': [120, None, 180]
})

products = pd.DataFrame({
    'ProductID': ['P1', 'P2', 'P3'],
    'ProductName': ['Widget', 'Gadget', 'Doohickey'],
    'Category': ['Tools', 'Electronics', 'Misc']
})

# Standardize dates
for df in [branch1, branch2]:
    df['PurchaseDate'] = pd.to_datetime(df['PurchaseDate'], errors='coerce')

# Concatenate sales
sales = pd.concat([branch1, branch2], ignore_index=True)

# Drop duplicates by TransactionID
sales = sales.drop_duplicates(subset=['TransactionID'])

# Handle missing dates and amounts
sales = sales.dropna(subset=['PurchaseDate'])
sales['Amount'] = sales['Amount'].fillna(sales['Amount'].median())

# Merge with products
full_data = sales.merge(products, on='ProductID', how='left')

print(full_data)</code></pre>
</div><h3 id="conclusion">Conclusion</h3><p>Combining and cleaning data from multiple sources is crucial for robust data analysis. This project demonstrated core pandas techniques to unify, clean, and validate data, preparing it for deeper insights or modeling. Mastering these skills ensures you can handle real-world messy data with confidence and accuracy.</p><p>Ready to explore the final project—an end-to-end data science pipeline using pandas?</p><div class = "chapter-navi-section">
<a href="#top" class="nav-button prev">↑</a>
<a href='pandas-for-data-science.htm#real-world-data-science-projects-with-pandas' class="nav-button toc-link">Index</a>
<a href="#bottom" class="nav-button next">↓</a>
</div>
<h2 id='project-5-end-to-end-data-science-pipeline-using-pandas'>15.5 Project 5: End-to-End Data Science Pipeline using Pandas</h2><p>In this final project, we’ll bring together all the concepts covered throughout this book into a comprehensive, reproducible data science pipeline using pandas. This includes data loading, cleaning, transformation, analysis, and exporting results. The goal is to demonstrate how to structure your code, handle real-world challenges, and produce meaningful insights in a clear, maintainable way.</p><h3 id="dataset-overview">Dataset Overview</h3><p>We will use a publicly available <strong>retail sales dataset</strong> (simulated for this example) that includes transaction records, customer info, and product details. This dataset features multiple sources and formats, missing values, and time series data. The pipeline will:</p><ul>
 <li>Load data from CSV and JSON files</li>
 <li>Clean and preprocess the data (handle missing data, type conversions)</li>
 <li>Perform feature engineering (date features, categorical encoding)</li>
 <li>Aggregate and analyze customer sales by segment</li>
 <li>Export cleaned and aggregated data for reporting</li>
</ul><h3 id="step-1-setup-and-data-loading">Step 1: Setup and Data Loading</h3><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import pandas as pd
import numpy as np

# Load datasets
sales = pd.read_csv('sales.csv')  # transaction data
customers = pd.read_json('customers.json')  # customer profiles
products = pd.read_csv('products.csv')  # product catalog

# Preview data
print(sales.head())
print(customers.head())
print(products.head())</code></pre>
</div><h3 id="step-2-data-cleaning-and-preprocessing">Step 2: Data Cleaning and Preprocessing</h3><p>Handle missing values, standardize formats, and ensure data types are correct.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Convert dates
sales['PurchaseDate'] = pd.to_datetime(sales['PurchaseDate'], errors='coerce')

# Drop sales records with missing PurchaseDate or Amount
sales = sales.dropna(subset=['PurchaseDate', 'Amount'])

# Fill missing customer IDs (if any) with placeholder
sales['CustomerID'] = sales['CustomerID'].fillna('Unknown').astype(str)
customers['CustomerID'] = customers['CustomerID'].astype(str)

# Normalize string columns
products['ProductName'] = products['ProductName'].str.strip().str.title()
customers['CustomerName'] = customers['CustomerName'].str.strip().str.title()

# Ensure IDs are strings
sales['ProductID'] = sales['ProductID'].astype(str)
products['ProductID'] = products['ProductID'].astype(str)</code></pre>
</div><h3 id="step-3-merging-datasets">Step 3: Merging Datasets</h3><p>Combine sales with customer and product information to enrich the dataset.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Merge sales with products
sales_products = sales.merge(products, on='ProductID', how='left')

# Merge with customer data
full_data = sales_products.merge(customers, on='CustomerID', how='left')

print(full_data.info())</code></pre>
</div><h3 id="step-4-feature-engineering">Step 4: Feature Engineering</h3><p>Create new features useful for analysis:</p><ul>
 <li>Extract year, month, weekday from purchase date</li>
 <li>Convert categorical fields to pandas <code>category</code> dtype to save memory and speed up grouping</li>
</ul><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">full_data['Year'] = full_data['PurchaseDate'].dt.year
full_data['Month'] = full_data['PurchaseDate'].dt.month
full_data['Weekday'] = full_data['PurchaseDate'].dt.day_name()

# Convert categories
for col in ['Category', 'CustomerSegment', 'Weekday']:
    if col in full_data.columns:
        full_data[col] = full_data[col].astype('category')

print(full_data.head())</code></pre>
</div><h3 id="step-5-aggregation-and-customer-segmentation">Step 5: Aggregation and Customer Segmentation</h3><p>Group customers by segment and analyze their total spending and transaction counts.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python"># Group sales by customer segment and month
segment_summary = (full_data
                   .groupby(['CustomerSegment', 'Year', 'Month'])
                   .agg(TotalSales=('Amount', 'sum'),
                        TransactionCount=('TransactionID', 'count'))
                   .reset_index())

print(segment_summary.head())</code></pre>
</div><h3 id="step-6-visualization-optional-requires-matplotlib">Step 6: Visualization (Optional, requires matplotlib)</h3><p>Quickly plot sales trends by customer segment.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">import matplotlib.pyplot as plt

for segment in segment_summary['CustomerSegment'].unique():
    subset = segment_summary[segment_summary['CustomerSegment'] == segment]
    plt.plot(pd.to_datetime(subset[['Year', 'Month']].assign(DAY=1)), 
             subset['TotalSales'], label=segment)

plt.title('Monthly Sales by Customer Segment')
plt.xlabel('Month')
plt.ylabel('Total Sales')
plt.legend()
plt.show()</code></pre>
</div><h3 id="step-7-export-cleaned-and-aggregated-data">Step 7: Export Cleaned and Aggregated Data</h3><p>Save processed datasets for reporting or further modeling.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">full_data.to_parquet('cleaned_retail_data.parquet')
segment_summary.to_csv('customer_segment_summary.csv', index=False)</code></pre>
</div><h3 id="step-8-wrap-as-functions-for-reproducibility">Step 8: Wrap as Functions for Reproducibility</h3><p>Organizing code in functions makes the pipeline modular and easy to maintain.</p><div class="code-block">
 <button class="copy-button" onclick="copyCode(this)">▯</button>
 <pre><code class="language-python">def load_data():
    sales = pd.read_csv('sales.csv')
    customers = pd.read_json('customers.json')
    products = pd.read_csv('products.csv')
    return sales, customers, products

def clean_data(sales, customers, products):
    sales['PurchaseDate'] = pd.to_datetime(sales['PurchaseDate'], errors='coerce')
    sales = sales.dropna(subset=['PurchaseDate', 'Amount'])
    sales['CustomerID'] = sales['CustomerID'].fillna('Unknown').astype(str)
    customers['CustomerID'] = customers['CustomerID'].astype(str)
    products['ProductName'] = products['ProductName'].str.strip().str.title()
    sales['ProductID'] = sales['ProductID'].astype(str)
    products['ProductID'] = products['ProductID'].astype(str)
    return sales, customers, products

def merge_data(sales, customers, products):
    sales_products = sales.merge(products, on='ProductID', how='left')
    full_data = sales_products.merge(customers, on='CustomerID', how='left')
    return full_data

def feature_engineering(full_data):
    full_data['Year'] = full_data['PurchaseDate'].dt.year
    full_data['Month'] = full_data['PurchaseDate'].dt.month
    full_data['Weekday'] = full_data['PurchaseDate'].dt.day_name()
    for col in ['Category', 'CustomerSegment', 'Weekday']:
        if col in full_data.columns:
            full_data[col] = full_data[col].astype('category')
    return full_data

def aggregate_data(full_data):
    return (full_data.groupby(['CustomerSegment', 'Year', 'Month'])
                    .agg(TotalSales=('Amount', 'sum'),
                         TransactionCount=('TransactionID', 'count'))
                    .reset_index())

def run_pipeline():
    sales, customers, products = load_data()
    sales, customers, products = clean_data(sales, customers, products)
    full_data = merge_data(sales, customers, products)
    full_data = feature_engineering(full_data)
    segment_summary = aggregate_data(full_data)
    full_data.to_parquet('cleaned_retail_data.parquet')
    segment_summary.to_csv('customer_segment_summary.csv', index=False)
    print("Pipeline complete. Files saved.")

if __name__ == "__main__":
    run_pipeline()</code></pre>
</div><h3 id="final-thoughts">Final Thoughts</h3><p>This end-to-end pipeline demonstrates:</p><ul>
 <li><strong>Data loading</strong> from various formats</li>
 <li><strong>Cleaning and preprocessing</strong> including date parsing and missing value handling</li>
 <li><strong>Merging datasets</strong> to enrich the base data</li>
 <li><strong>Feature engineering</strong> with datetime and categorical data</li>
 <li><strong>Aggregation and segmentation</strong> to extract insights</li>
 <li><strong>Exporting results</strong> for sharing or further analysis</li>
 <li><strong>Modular, reusable code</strong> structure</li>
</ul><p>Using pandas effectively in such pipelines boosts productivity, ensures reproducibility, and prepares data science practitioners for real-world challenges. The techniques demonstrated here form the foundation for more advanced projects, including machine learning and real-time analytics.</p><p>Feel free to adapt and expand this pipeline with domain-specific transformations, richer visualizations, or integration with external APIs and databases for full-scale production workflows.</p><div class = "chapter-navi-section">
<a href="pandas-for-data-science-customizing-pandas-with-extensions-and-apis.htm" class="nav-button prev">←</a>
<a href='pandas-for-data-science.htm#real-world-data-science-projects-with-pandas' class="nav-button toc-link">Index</a>
<a href="" class="nav-button next"></a>
</div>
<section class="download-section">
  <div class="book-cover" style="background-color: #f9f6f1;border: 1px solid #ccc;"> 
    <div class="book-cover-title" style="font-size: 18px;color: #333;top: 20px;">Pandas for Data Science</div>
    <div class="book-cover-author">readbytes</div>
    <div class="book-cover-bottom-stripe" style="background-color: #333;"></div>
    <img class="book-cover-icon" style="width: 90px;height: 90px;top: 70px;" src="data:image/png;base64,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"/>
  </div>
  <div class="download-info">
    <h2 id='bottom'>Download This Ebook</h2>
    <p class="format-label">Available formats:</p>
    <div class="download-buttons">
      <a href="pandas-for-data-science.pdf" class="download-button">📄 PDF</a>
      <a href="pandas-for-data-science.epub" class="download-button">📘 EPUB</a>
    </div>
  </div>
</section><div class='related-books'>
<h2>Python Data Science Books</h2>
<div class='related-book-list'>
<a href="pandas-for-data-science.htm">
<div class="book-cover" style="background-color: #f9f6f1;border: 1px solid #ccc;"> 
    <div class="book-cover-title" style="font-size: 18px;color: #333;top: 20px;">Pandas for Data Science</div>
    <div class="book-cover-author">readbytes</div>
    <div class="book-cover-bottom-stripe" style="background-color: #333;"></div>
    <img class="book-cover-icon" style="width: 90px;height: 90px;top: 70px;" src="data:image/png;base64,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"/>
  </div>
</a>



<a href='python.htm'><div class='book-cover' style='background-color: #f9f6f1;color: #111111;border: 1px solid #ccc;display: flex;justify-content: center;align-items: center;text-align: center;'>More Python Books</div></a>
</div>
</div><script>
function copyCode(button) {const code = button.nextElementSibling.innerText;navigator.clipboard.writeText(code).then(() => {button.textContent = 'Copied!';setTimeout(() => { button.textContent = 'Copy'; }, 1500);});}
</script></div>
<style>
.site-footer {margin-top: 60px;padding: 20px 0;border-top: 1px solid #eee;text-align: center;font-size: 14px;}
.site-footer a {text-decoration: none;}
.light-mode .site-footer {color: #777;border-color: #eee;}
.dark-mode .site-footer {color: #888;border-color: #333;}
.light-mode .site-footer a {color: #555;}
.dark-mode .site-footer a {color: #aaa;}
.site-footer a:hover {text-decoration: underline;}    
</style>
<footer class="site-footer">
<nav><a href="https://readbytes.github.io">Home</a> |Email:<script type="text/javascript">
var part1 = 'yinpeng';var part6 = '263';var part2 = Math.pow(2,6);var part3 = String.fromCharCode(part2);var part4 = 'hotmail dot com';var part5 = part1 + String.fromCharCode(part2) + part4;document.write(part1 + part6 + part3 + part4);
</script>
| <a id="mode-toggle" href="#">Toggle Dark Mode</a>
</nav><p>© 2025 — All rights reserved.</p></footer>
<script>
const toggleButton = document.getElementById('mode-toggle');
const body = document.body;
const savedMode = localStorage.getItem('mode') || 'light';
body.classList.add(savedMode + '-mode');
toggleButton.addEventListener('click', () => {
      const isDark = body.classList.contains('dark-mode');
      body.classList.toggle('dark-mode', !isDark);
      body.classList.toggle('light-mode', isDark);
      localStorage.setItem('mode', isDark ? 'light' : 'dark');
});
function copyCode(button) {const code = button.nextElementSibling.innerText;navigator.clipboard.writeText(code).then(() => {button.textContent = 'Copied!';setTimeout(() => { button.textContent = 'Copy'; }, 1500);});}
</script>
</body>
</html>